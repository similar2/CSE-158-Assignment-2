{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2669d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: ./archive/\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math, random\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# base path for data files\n",
    "base_path = \"../archive/\"\n",
    "\n",
    "print(f\"Working directory set to: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3203d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1: EXPLORATORY ANALYSIS ---\n",
      "Warning: RAW files not found. Skipping EDA.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2.1: EDA - Load Data & Compute Stats ===\n",
    "print(\"\\n--- PART 1: EXPLORATORY ANALYSIS ---\")\n",
    "\n",
    "try:\n",
    "    raw_inter_df = pd.read_csv(base_path + \"RAW_interactions.csv\")\n",
    "    raw_recipes_df = pd.read_csv(base_path + \"RAW_recipes.csv\")\n",
    "\n",
    "    num_users = raw_inter_df[\"user_id\"].nunique()\n",
    "    num_items = raw_inter_df[\"recipe_id\"].nunique()\n",
    "    sparsity = len(raw_inter_df) / (num_users * num_items)\n",
    "    print(f\"Sparsity: {sparsity:.6f}\")\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    raw_inter_df[\"rating\"].hist(bins=10, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.title(\"Ratings Distribution\")\n",
    "\n",
    "    raw_recipes_df[\"n_ingredients\"] = raw_recipes_df[\"n_ingredients\"].fillna(0)\n",
    "    raw_recipes_df[\"minutes\"] = raw_recipes_df[\"minutes\"].fillna(0)\n",
    "    raw_recipes_df[\"calories\"] = raw_recipes_df[\"nutrition\"].apply(\n",
    "        lambda x: float(ast.literal_eval(x)[0]) if isinstance(x, str) else 0\n",
    "    )\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    raw_recipes_df[\"n_ingredients\"].hist(bins=30, color=\"orange\", edgecolor=\"black\")\n",
    "    plt.title(\"# Ingredients\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    np.log1p(raw_recipes_df[\"minutes\"]).hist(bins=30, color=\"green\", edgecolor=\"black\")\n",
    "    plt.title(\"Time (Log Scale)\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    np.log1p(raw_recipes_df[\"calories\"]).hist(bins=30, color=\"red\", edgecolor=\"black\")\n",
    "    plt.title(\"Calories (Log Scale)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: RAW files not found. Skipping EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47990b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3.1: Load Official Splits & ID mappings ===\n",
    "print(\"\\n--- PART 2: MODELING PIPELINE ---\")\n",
    "print(\"Loading Official Train/Test Splits...\")\n",
    "\n",
    "train_df = pd.read_csv(base_path + \"interactions_train.csv\")\n",
    "test_df = pd.read_csv(base_path + \"interactions_test.csv\")\n",
    "pp_recipes = pd.read_csv(base_path + \"PP_recipes.csv\")\n",
    "\n",
    "if \"raw_recipes_df\" not in locals():\n",
    "    raw_recipes_df = pd.read_csv(base_path + \"RAW_recipes.csv\")\n",
    "    raw_recipes_df[\"n_ingredients\"] = raw_recipes_df[\"n_ingredients\"].fillna(0)\n",
    "    raw_recipes_df[\"minutes\"] = raw_recipes_df[\"minutes\"].fillna(0)\n",
    "    raw_recipes_df[\"calories\"] = raw_recipes_df[\"nutrition\"].apply(\n",
    "        lambda x: float(ast.literal_eval(x)[0]) if isinstance(x, str) else 0\n",
    "    )\n",
    "\n",
    "i_to_raw_id = pd.Series(pp_recipes.id.values, index=pp_recipes.i).to_dict()\n",
    "pp_recipes[\"ingr_ids_list\"] = pp_recipes[\"ingredient_ids\"].apply(ast.literal_eval)\n",
    "i_to_ingr_set = pd.Series(\n",
    "    pp_recipes[\"ingr_ids_list\"].values, index=pp_recipes.i\n",
    ").to_dict()\n",
    "\n",
    "raw_recipes_df[\"tags_list\"] = raw_recipes_df[\"tags\"].apply(\n",
    "    lambda x: set(ast.literal_eval(x))\n",
    ")\n",
    "raw_id_to_tags = pd.Series(\n",
    "    raw_recipes_df[\"tags_list\"].values, index=raw_recipes_df[\"id\"]\n",
    ").to_dict()\n",
    "\n",
    "print(\"Normalizing Content Features...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6632c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3.2: Feature Normalization & Item Mapping ===\n",
    "raw_recipes_df[\"log_minutes\"] = np.log1p(raw_recipes_df[\"minutes\"])\n",
    "raw_recipes_df[\"log_calories\"] = np.log1p(raw_recipes_df[\"calories\"])\n",
    "raw_recipes_df[\"log_n_ingr\"] = np.log1p(raw_recipes_df[\"n_ingredients\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cols = [\"log_minutes\", \"log_calories\", \"log_n_ingr\"]\n",
    "norm_feats = scaler.fit_transform(raw_recipes_df[cols])\n",
    "\n",
    "raw_id_to_feats = {rid: norm_feats[idx] for idx, rid in enumerate(raw_recipes_df[\"id\"]) }\n",
    "\n",
    "item_content_feats = {}\n",
    "item_tag_sets = {}\n",
    "valid_items = []\n",
    "\n",
    "for i, raw_id in i_to_raw_id.items():\n",
    "    if raw_id in raw_id_to_feats:\n",
    "        item_content_feats[i] = raw_id_to_feats[raw_id]\n",
    "        valid_items.append(i)\n",
    "    if raw_id in raw_id_to_tags:\n",
    "        item_tag_sets[i] = raw_id_to_tags[raw_id]\n",
    "\n",
    "item_counts = train_df[\"i\"].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ef74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3.3: Build User Profiles ===\n",
    "print(\"\\n--- PART 3: TRAINING MODEL ---\")\n",
    "\n",
    "print(\"Building User Profiles...\")\n",
    "user_centroids = {}\n",
    "user_ingr_sets = defaultdict(set)\n",
    "user_tag_sets = defaultdict(set)\n",
    "user_sums = defaultdict(lambda: np.zeros(3))\n",
    "user_cnts = defaultdict(int)\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    u = row[\"u\"]\n",
    "    i = row[\"i\"]\n",
    "    if i in item_content_feats:\n",
    "        user_sums[u] += item_content_feats[i]\n",
    "        user_cnts[u] += 1\n",
    "    if i in i_to_ingr_set:\n",
    "        user_ingr_sets[u].update(i_to_ingr_set[i])\n",
    "    if i in item_tag_sets:\n",
    "        user_tag_sets[u].update(item_tag_sets[i])\n",
    "\n",
    "for u in user_sums:\n",
    "    if user_cnts[u] > 0:\n",
    "        user_centroids[u] = user_sums[u] / user_cnts[u]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3.4: Generate Training Samples ===\n",
    "print(\"Generating Training Samples...\")\n",
    "X_train = []\n",
    "y_train = []\n",
    "train_sample = train_df.sample(n=50000, random_state=42)\n",
    "\n",
    "def get_content_vector(u, i):\n",
    "    jaccard_ingr = 0.0\n",
    "    if u in user_ingr_sets and i in i_to_ingr_set:\n",
    "        u_s = user_ingr_sets[u]\n",
    "        i_s = i_to_ingr_set[i]\n",
    "        if u_s and i_s:\n",
    "            jaccard_ingr = len(u_s.intersection(i_s)) / len(u_s.union(i_s))\n",
    "\n",
    "    jaccard_tags = 0.0\n",
    "    if u in user_tag_sets and i in item_tag_sets:\n",
    "        u_t = user_tag_sets[u]\n",
    "        i_t = item_tag_sets[i]\n",
    "        if u_t and i_t:\n",
    "            jaccard_tags = len(u_t.intersection(i_t)) / len(u_t.union(i_t))\n",
    "\n",
    "    if u in user_centroids and i in item_content_feats:\n",
    "        diff = np.abs(user_centroids[u] - item_content_feats[i])\n",
    "    else:\n",
    "        diff = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "    return np.concatenate(([jaccard_ingr, jaccard_tags], diff))\n",
    "\n",
    "for _, row in train_sample.iterrows():\n",
    "    u = row[\"u\"]\n",
    "    pos_i = row[\"i\"]\n",
    "    if u not in user_centroids:\n",
    "        continue\n",
    "    X_train.append(get_content_vector(u, pos_i))\n",
    "    y_train.append(1)\n",
    "    neg_i = random.choice(valid_items)\n",
    "    while neg_i == pos_i:\n",
    "        neg_i = random.choice(valid_items)\n",
    "    X_train.append(get_content_vector(u, neg_i))\n",
    "    y_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3.5: Fit Model ===\n",
    "print(\"Fitting Logistic Regression...\")\n",
    "model = LogisticRegression(class_weight=\"balanced\", max_iter=1000, C=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "weights = model.coef_[0]\n",
    "print(\"--- Model Weights Interpretation ---\")\n",
    "print(f\"Ingr Jaccard:   {weights[0]:.4f} (Positive = Users like shared ingredients)\")\n",
    "print(\n",
    "    f\"Tag Jaccard:    {weights[1]:.4f}) (Should be Positive, but the Users apparently appreciate variety??\"\n",
    ")\n",
    "print(f\"Time Diff:      {weights[2]:.4f} (Negative = Users dislike Time mismatch)\")\n",
    "print(f\"Calorie Diff:   {weights[3]:.4f} (Negative = Users dislike Calorie mismatch)\")\n",
    "print(\n",
    "    f\"Complexity Diff:{weights[4]:.4f} (Negative = Users dislike Complexity mismatch)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1500859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4.1: Evaluation Functions ===\n",
    "print(\"\\n--- PART 4: EVALUATION ---\")\n",
    "\n",
    "sorted_items = sorted(item_counts, key=item_counts.get, reverse=True)\n",
    "global_top_10 = set(sorted_items[:10])\n",
    "\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_all_metrics(test_data, limit=1000):\n",
    "    total_users = 0\n",
    "    sum_hr_model = 0.0\n",
    "    sum_ndcg_model = 0.0\n",
    "    sum_auc_model = 0.0\n",
    "    sum_hr_pop = 0.0\n",
    "    sum_ndcg_pop = 0.0\n",
    "    sum_auc_pop = 0.0\n",
    "\n",
    "    print(f\"Evaluating on random subset of {limit} users...\")\n",
    "    print(\"Protocol: For each user, rank the True Item against 99 Random Decoys.\")\n",
    "\n",
    "    subset = test_data.sample(n=limit, random_state=42)\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        u = row[\"u\"]\n",
    "        true_item = row[\"i\"]\n",
    "        is_cold_start = u not in user_centroids\n",
    "        candidates = [true_item]\n",
    "        labels = [1]\n",
    "        while len(candidates) < 100:\n",
    "            decoy = random.choice(valid_items)\n",
    "            if decoy != true_item:\n",
    "                candidates.append(decoy)\n",
    "                labels.append(0)\n",
    "\n",
    "        model_scores = []\n",
    "        pop_scores = []\n",
    "\n",
    "        for item in candidates:\n",
    "            if not is_cold_start:\n",
    "                vec = get_content_vector(u, item).reshape(1, -1)\n",
    "                score = model.predict_proba(vec)[0][1]\n",
    "            else:\n",
    "                score = item_counts.get(item, 0)\n",
    "            model_scores.append(score)\n",
    "            pop_scores.append(item_counts.get(item, 0))\n",
    "\n",
    "        def calc_user_metrics(scores, current_labels):\n",
    "            zipped = list(zip(scores, current_labels))\n",
    "            zipped.sort(key=lambda x: x[0], reverse=True)\n",
    "            rank = -1\n",
    "            for r, (score, label) in enumerate(zipped):\n",
    "                if label == 1:\n",
    "                    rank = r + 1\n",
    "                    break\n",
    "            hr = 1 if rank <= 10 else 0\n",
    "            ndcg = 1 / math.log2(rank + 1) if rank <= 10 else 0\n",
    "            try:\n",
    "                auc = roc_auc_score(current_labels, scores)\n",
    "            except ValueError:\n",
    "                auc = 0.5\n",
    "            return hr, ndcg, auc\n",
    "\n",
    "        m_hr, m_ndcg, m_auc = calc_user_metrics(model_scores, labels)\n",
    "        sum_hr_model += m_hr\n",
    "        sum_ndcg_model += m_ndcg\n",
    "        sum_auc_model += m_auc\n",
    "\n",
    "        p_hr, p_ndcg, p_auc = calc_user_metrics(pop_scores, labels)\n",
    "        sum_hr_pop += p_hr\n",
    "        sum_ndcg_pop += p_ndcg\n",
    "        sum_auc_pop += p_auc\n",
    "\n",
    "        total_users += 1\n",
    "\n",
    "    return {\n",
    "        \"Model\": {\n",
    "            \"HR@10\": sum_hr_model / total_users,\n",
    "            \"NDCG@10\": sum_ndcg_model / total_users,\n",
    "            \"AUC\": sum_auc_model / total_users,\n",
    "        },\n",
    "        \"Popularity\": {\n",
    "            \"HR@10\": sum_hr_pop / total_users,\n",
    "            \"NDCG@10\": sum_ndcg_pop / total_users,\n",
    "            \"AUC\": sum_auc_pop / total_users,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4.2: Run Evaluation ===\n",
    "results = evaluate_all_metrics(test_df, limit=1000)\n",
    "\n",
    "print(f\"\\n--- FINAL RESULTS ---\")\n",
    "print(f\"{'Metric':<10} | {'Popularity':<10} | {'Our Model':<10}\")\n",
    "print(\"-\" * 35)\n",
    "print(\n",
    "    f\"{'HR@10':<10} | {results['Popularity']['HR@10']:.4f}     | {results['Model']['HR@10']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"{'NDCG@10':<10} | {results['Popularity']['NDCG@10']:.4f}     | {results['Model']['NDCG@10']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"{'AUC':<10}   | {results['Popularity']['AUC']:.4f}     | {results['Model']['AUC']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4.3: Plotting & Additional Visualizations ===\n",
    "# VISUALIZATION OF RESULTS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_final_metrics(results):\n",
    "    metrics = [\"HR@10\", \"NDCG@10\", \"AUC\"]\n",
    "    pop_vals = [results[\"Popularity\"][m] for m in metrics]\n",
    "    model_vals = [results[\"Model\"][m] for m in metrics]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    rects1 = ax1.bar(x - width / 2, pop_vals, width, label=\"Popularity\", color=\"gray\", alpha=0.7)\n",
    "    rects2 = ax1.bar(x + width / 2, model_vals, width, label=\"Our Model\", color=\"#1f77b4\")\n",
    "\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.set_title(\"Baseline vs. Content Model Performance\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(metrics)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, max(model_vals) * 1.2)\n",
    "\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax1.annotate(f\"{height:.2f}\",\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"center\", va=\"bottom\")\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "\n",
    "    print(\"Generating Hit Rate Curve (this takes a moment)...\")\n",
    "\n",
    "    def get_raw_ranks(limit=500):\n",
    "        ranks_m = []\n",
    "        ranks_p = []\n",
    "        subset = test_df.sample(n=limit, random_state=42)\n",
    "        for _, row in subset.iterrows():\n",
    "            u, true_item = row[\"u\"], row[\"i\"]\n",
    "            is_cold = u not in user_centroids\n",
    "            candidates = [true_item]\n",
    "            while len(candidates) < 100:\n",
    "                decoy = random.choice(valid_items)\n",
    "                if decoy != true_item:\n",
    "                    candidates.append(decoy)\n",
    "\n",
    "            m_scores = []\n",
    "            p_scores = []\n",
    "            for item in candidates:\n",
    "                if not is_cold:\n",
    "                    vec = get_content_vector(u, item).reshape(1, -1)\n",
    "                    m_scores.append(model.predict_proba(vec)[0][1])\n",
    "                else:\n",
    "                    m_scores.append(item_counts.get(item, 0))\n",
    "                p_scores.append(item_counts.get(item, 0))\n",
    "\n",
    "            zipped_m = sorted(zip(m_scores, range(100)), key=lambda x: x[0], reverse=True)\n",
    "            for rank, (score, idx) in enumerate(zipped_m):\n",
    "                if idx == 0:\n",
    "                    ranks_m.append(rank + 1)\n",
    "                    break\n",
    "\n",
    "            zipped_p = sorted(zip(p_scores, range(100)), key=lambda x: x[0], reverse=True)\n",
    "            for rank, (score, idx) in enumerate(zipped_p):\n",
    "                if idx == 0:\n",
    "                    ranks_p.append(rank + 1)\n",
    "                    break\n",
    "        return ranks_m, ranks_p\n",
    "\n",
    "    raw_ranks_model, raw_ranks_pop = get_raw_ranks(limit=500)\n",
    "\n",
    "    k_values = range(1, 21)\n",
    "    hr_curve_m = [sum(1 for r in raw_ranks_model if r <= k)/len(raw_ranks_model) for k in k_values]\n",
    "    hr_curve_p = [sum(1 for r in raw_ranks_pop if r <= k)/len(raw_ranks_pop) for k in k_values]\n",
    "\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.plot(k_values, hr_curve_m, marker=\"o\", linewidth=2, label=\"Our Model\")\n",
    "    ax2.plot(k_values, hr_curve_p, marker=\"x\", linestyle=\"--\", color=\"gray\", label=\"Popularity\")\n",
    "\n",
    "    ax2.set_xlabel(\"K (Top-K Recommendations)\")\n",
    "    ax2.set_ylabel(\"Hit Rate\")\n",
    "    ax2.set_title(\"Hit Rate @ K (Curve)\")\n",
    "    ax2.set_xticks([1, 5, 10, 15, 20])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_final_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb506ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4.4: VISUALIZATION - Top Popular Items ===\n",
    "def show_most_popular_items(top_n=20):\n",
    "    print(f\"--- TOP {top_n} MOST POPULAR RECIPES (BASELINE) ---\")\n",
    "    print(\"Building ID maps...\")\n",
    "\n",
    "    df_raw = raw_recipes_df\n",
    "    local_i_to_raw = pd.Series(pp_recipes[\"id\"].values, index=pp_recipes[\"i\"]).to_dict()\n",
    "    local_raw_to_name = pd.Series(df_raw[\"name\"].values, index=df_raw[\"id\"]).to_dict()\n",
    "\n",
    "    top_items = train_df[\"i\"].value_counts().head(top_n)\n",
    "\n",
    "    print(f\"\\n{'Rank':<5} | {'Count':<8} | {'Recipe Name'}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for rank, (i, count) in enumerate(top_items.items()):\n",
    "        if i in local_i_to_raw:\n",
    "            raw_id = local_i_to_raw[i]\n",
    "            name = local_raw_to_name.get(raw_id, \"Unknown Name\")\n",
    "            if isinstance(name, str) and len(name) > 45:\n",
    "                name = name[:42] + \"...\"\n",
    "            print(f\"#{rank+1:<4} | {count:<8} | {name}\")\n",
    "        else:\n",
    "            print(f\"#{rank+1:<4} | {count:<8} | [ID {i} not found]\")\n",
    "\n",
    "top_items = train_df[\"i\"].value_counts().head(20)\n",
    "show_most_popular_items(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeafbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4.5: VISUALIZATION - Popularity Dropoff ===\n",
    "def show_popularity_dropoff(top_n=20):\n",
    "    print(f\"\\n--- POPULARITY DROPOFF ANALYSIS (Top {top_n} Train Items) ---\")\n",
    "\n",
    "    top_train_items = pd.Series(item_counts).sort_values(ascending=False).head(top_n)\n",
    "    test_counts_series = test_df[\"i\"].value_counts()\n",
    "    raw_id_to_name = pd.Series(\n",
    "        raw_recipes_df[\"name\"].values, index=raw_recipes_df[\"id\"]\n",
    "    ).to_dict()\n",
    "\n",
    "    print(\n",
    "        f\"{'Rank':<5} | {'Recipe Name (Train Leader)':<40} | {'Train Reviews':<12} | {'Test Reviews'}\"\n",
    "    )\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for rank, (i, train_count) in enumerate(top_train_items.items()):\n",
    "        test_count = test_counts_series.get(i, 0)\n",
    "        if i in i_to_raw_id:\n",
    "            raw_id = i_to_raw_id[i]\n",
    "            name = raw_id_to_name.get(raw_id, \"Unknown Name\")\n",
    "            if len(name) > 30:\n",
    "                name = name[:25] + \"...\"\n",
    "        else:\n",
    "            name = f\"ID {i}\"\n",
    "        alert = \" <--- zero times!\" if test_count == 0 else \"\"\n",
    "        print(f\"#{rank+1:<4} | {name:<40} | {train_count:<12} | {test_count}{alert}\")\n",
    "\n",
    "show_popularity_dropoff(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
